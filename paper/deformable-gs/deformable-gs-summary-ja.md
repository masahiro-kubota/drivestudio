# Deformable 3D Gaussians - 日本語要約

## 論文情報

**タイトル**: Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction
**著者**: Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, Xiaogang Jin
**所属**: 浙江大学、ByteDance Inc.

---

## Abstract（概要）

本論文は、単眼動的シーンの高品質再構成のための新しい手法を提案している。従来の暗黙的ニューラル表現（implicit neural representation）を用いた動的レンダリング手法は、シーン内のオブジェクトの細部を捉えることが困難で、リアルタイムレンダリングも難しかった。

この課題に対し、**Deformable 3D Gaussians Splatting**という手法を提案：
- 3D Gaussiansを用いてシーンを再構成
- 変形フィールド（deformation field）を使用して正準空間（canonical space）で3D Gaussiansを学習
- 単眼動的シーンをモデル化

さらに、**アニーリングスムージングトレーニングメカニズム（annealing smoothing training mechanism）**を導入し、実世界データセットにおける不正確なポーズの影響を軽減。

### 主な成果
- レンダリング品質と速度の両面で既存手法を大幅に上回る
- 新規視点合成、時間補間、リアルタイムレンダリングに適している

---

## 1. Introduction（導入）

### 背景
動的シーンの高品質再構成とフォトリアリスティックレンダリングは、以下の用途で重要：
- AR/VR（拡張現実/仮想現実）
- 3Dコンテンツ制作
- エンターテインメント

### 従来手法の課題

**メッシュベースの手法**（従来）:
- ディテールやリアリズムの欠如
- セマンティック情報の不足
- トポロジー変化への対応困難

**NeRFベースの手法**（暗黙的表現）:
- 静的シーンでは成功（Instant-NGP、zipNeRF、3D-GSなど）
- 動的シーンでは以下の問題：
  - 収束が遅い
  - オーバーフィッティングしやすい
  - リアルタイムレンダリングが困難

### 本研究のアプローチ

**静的3D-GSの拡張**:
- 単眼動的シーンのモデリングフレームワーク
- フレームごとの再構成ではなく、時間を条件とした3D Gaussiansを使用
- 変形フィールドと正準空間の3D Gaussiansを共同学習

**主な貢献**:
1. リアルタイムレンダリングと高忠実度シーン再構成を実現する変形可能3D-GSフレームワーク
2. 動的ディテールを保ちながら時間的スムーズネスを確保する新しいアニーリングスムージングトレーニングメカニズム
3. 変形フィールドを通じて3D-GSを動的シーンに拡張する初のフレームワーク

---

## 2. Related Work（関連研究）

### 2.1. Neural Rendering for Dynamic Scenes（動的シーンのニューラルレンダリング）

**NeRFの拡張**:
- NeRFは静的シーンで成功を収めた
- 動的シーンへの拡張が重要な研究テーマ

**動的NeRFのアプローチ**:

1. **Entangled methods**（結合型手法）:
   - 時間変数tを追加入力としてNeRFに条件付け
   - 問題: 幾何学的事前情報が不足

2. **Disentangled methods**（分離型手法）:
   - 変形フィールドを使用して正準空間にマッピング
   - 時間と放射輝度フィールドを分離
   - 剛体運動や トポロジー変化に対応可能

**その他の改良手法**:
- 静的・動的オブジェクトのセグメンテーション
- 深度情報の組み込み
- 2D CNNによるシーン事前情報のエンコード
- マルチビュービデオの冗長情報活用

### 2.2. Acceleration of Neural Rendering（ニューラルレンダリングの高速化）

**Pre-computed methods**（事前計算手法）:
- 球面調和関数係数や特徴ベクトルを使用
- 推論速度は速いが、ストレージ容量が大きい

**Hybrid methods**（ハイブリッド手法）:
- 明示的グリッドにニューラルコンポーネントを組み込む
- トレーニングと推論の両方を高速化
- 例: グリッドベースやプレーンベースの構造

**Point-based methods**（ポイントベース手法）:
- 3D-GS: カスタムCUDA実装で高速レンダリング
- リアルタイムレンダリング（100 FPS以上）
- ただし、静的シーン用に設計されている

### 本研究の位置づけ
3D-GSのポイントベースレンダリングフレームワークを活用し、動的モデリングのトレーニングとレンダリング速度を高速化。

---

## 3. Method（手法）

### 概要（Figure 2参照）

**入力**:
- 単眼動的シーンの画像セット
- 時間ラベル
- SfMで較正されたカメラポーズとスパース点群

**3D Gaussians**:
- 中心位置 x
- 不透明度 σ
- 四元数 r とスケーリング s から得られる3D共分散行列 Σ
- 球面調和関数（SH）による視点依存の外観表現

### 3.1. Differentiable Rendering Through 3D Gaussians Splatting in Canonical Space

**微分可能なレンダリング**:

2D共分散行列への投影:
```
Σ' = J V Σ V^T J^T
```
- J: アフィン近似のヤコビアン
- V: ビュー行列（ワールド→カメラ座標）
- Σ: 3D共分散行列

3D共分散行列の分解:
```
Σ = RSS^T R^T
```
- R: 回転行列（四元数rから）
- S: スケーリング行列（ベクトルsから）

ピクセル色のレンダリング:
```
C(p) = Σ T_i α_i c_i
```
- T_i: 透過率
- α_i: ガウシアンのアルファ値
- c_i: ガウシアンの色

**Adaptive Density Control**（適応的密度制御）:
1. **Pruning**（枝刈り）: 不透明度が低い透明なガウシアンを削除
2. **Densification**（高密度化）:
   - 細かい幾何学的な領域: ガウシアンをクローン
   - 大きく重なる領域: ガウシアンを分割

**正準空間での学習**:
- 時間条件付き学習可能パラメータを各3D Gaussianに適用するのではない
- 正準空間で3D Gaussiansを学習し、変形フィールドで位置と形状の変化を学習

### 3.2. Deformable 3D Gaussians（変形可能3D Gaussians）

**モチベーション**:
- 時間依存のビューコレクションごとに3D-GSを個別にトレーニングするアプローチは、離散時間には適しているが連続的な単眼キャプチャには不十分

**アプローチ**:
- 3D Gaussiansとともに変形フィールドを共同学習
- 運動と幾何構造を分離

**変形ネットワーク**:
```
(δx, δr, δs) = F_θ(γ(sg(x)), γ(t))
```
- sg(·): stop-gradient操作
- γ: 位置エンコーディング
- F_θ: MLPベースの変形ネットワーク

位置エンコーディング:
```
γ(p) = (sin(2^k πp), cos(2^k πp))_{k=0}^{L-1}
```
- 合成シーン: L=10 (x用), L=6 (t用)
- 実シーン: L=10 (xとt両方)

**ネットワーク構造**:
- 深さ D = 8
- 隠れ層の次元 W = 256
- グリッド/プレーン構造は使用しない（低ランク仮定が動的シーンに適さないため）

### 3.3. Annealing Smooth Training（アニーリングスムージングトレーニング）

**課題**:
実世界データセットではポーズ推定の不正確さが問題となる：
- トレーニングデータへのオーバーフィッティング
- フレーム間の空間的なジッター
- 時間補間タスクでの不規則なレンダリングジッター

**提案手法 - AST（Annealing Smooth Training）**:
```
Δ = F_θ(γ(sg(x)), γ(t) + X(i))
X(i) = N(0,1) · β · Δt · (1 - i/τ)
```
- X(i): i番目のトレーニングイテレーションでの線形減衰ガウシアンノイズ
- N(0,1): 標準ガウシアン分布
- β: スケーリング係数（0.1）
- Δt: 平均時間間隔
- τ: しきい値イテレーション（20k）

**利点**:
- 追加の計算オーバーヘッドなし
- トレーニング初期段階でモデルの時間的汎化を向上
- 後期段階で過度なスムージングを防ぎ、動的シーンのディテールを保持
- 時間補間タスクでのジッターを軽減

---

## 4. Experiment（実験）

### 4.1. Implementation Details（実装詳細）

**フレームワーク**:
- PyTorch実装
- 深度可視化を組み込んだ微分可能ガウシアンラスタライゼーション

**トレーニング**:
- 総イテレーション: 40k
- 初期3kイテレーション: 3D Gaussiansのみトレーニング（安定した位置と形状を得るため）
- その後: 3D Gaussiansと変形フィールドを共同トレーニング

**最適化**:
- オプティマイザ: Adam
- 3D Gaussians: 公式実装と同じ学習率
- 変形ネットワーク: 指数減衰（8e-4 → 1.6e-6）
- Adam β値: (0.9, 0.999)

**データセット設定**:
- 合成データセット: 黒背景、800x800解像度
- GPU: NVIDIA RTX 3090

### 4.2. Results and Comparisons（結果と比較）

**合成データセットでの比較**:
- ベースライン: 3D-GS、D-NeRF、TiNeuVox、Tensor4D、K-Planes
- 評価指標: PSNR、SSIM、LPIPS

主な結果（Table 1）:
- LPIPS、SSIMなどの構造的一貫性指標で顕著な優位性
- 高忠実度動的シーンモデリングを実現
- 新規視点レンダリングで詳細を捉える

**実世界データセットでの比較**:
- データセット: NeRF-DS、HyperNeRF
- HyperNeRFは一部カメラポーズが不正確
- NeRF-DSでの定量的・定性的評価で優位性を示す

**レンダリング効率**:
- 3D Gaussians数が250k未満の場合、30 FPS以上のリアルタイムレンダリングを達成（NVIDIA RTX 3090）

**深度可視化**:
- 合成データセットで深度可視化を実施
- 変形ネットワークが色ベースのハードコーディングではなく、時間的変換を適切に生成していることを確認

### 4.3. Ablation Study（アブレーション研究）

**Annealing Smooth Training**:
- 実世界データセットでのオーバーフィッティングを効果的に軽減
- 変形フィールドの時間的スムーズネスを大幅に向上
- 詳細への収束を改善

**その他のアブレーション**（補足資料）:
- ネットワークアーキテクチャ
- 背景色の影響
- SE(3)変形フィールドの使用

---

## 5. Limitations（制限事項）

### 実験から明らかになった制限

1. **視点の多様性への依存**:
   - スパースな視点や限定的な視点カバレッジのデータセットでオーバーフィッティングの可能性

2. **ポーズ推定精度への依存**:
   - NeRF-DS/HyperNeRFデータセットでCOLMAPによるポーズ推定のずれが原因で最適なPSNRを達成できなかった

3. **計算複雑性**:
   - 時間的複雑性が3D Gaussians数に比例
   - 大量の3D Gaussiansの場合、トレーニング時間とメモリ消費が増加

4. **動きの範囲**:
   - 中程度の動きのシーンで主に評価
   - 微妙な表情など極端な人間の動きへの適応性は未検証

---

## 6. Conclusions（結論）

### 主な成果

本研究では、単眼動的シーンモデリング用の新しい**変形可能3D Gaussian splatting手法**を導入：

1. **品質と速度の両立**:
   - 既存手法を品質と速度の両面で上回る

2. **正準空間での学習**:
   - 3D Gaussiansを正準空間で学習することで、動的にキャプチャされた単眼シーンに対する3D-GS微分可能レンダリングパイプラインの汎用性を向上

3. **ポイントベース手法の優位性**:
   - 暗黙的表現と比較して、ポスト プロダクションタスクにより適している

4. **Annealing Smooth Training**:
   - 時間エンコーディングに関連するオーバーフィッティングを削減
   - 詳細なシーンディテールを維持
   - 追加のトレーニングオーバーヘッドなし

5. **リアルタイムレンダリング**:
   - 優れたレンダリング効果とリアルタイムレンダリング能力を両立

---

## 補足資料（Appendix）

### A. 実装詳細

**変形フィールドのネットワークアーキテクチャ**:
- MLP: (γ(x), γ(t)) → (δx, δr, δs)
- 8層の全結合層（ReLU活性化、256次元隠れ層）
- 256次元特徴ベクトルを出力
- 3つの追加全結合層（活性化なし）で位置、回転、スケーリングのオフセットを出力
- NeRF同様、4層目で特徴ベクトルと入力を連結
- ストレージ: 3D Gaussiansに対して追加2MBのみ

**最適化損失**:
```
L = (1-λ)L_1 + λL_{D-SSIM}
```
- λ = 0.2（全実験で使用）

### B. 追加結果

**NeRF-DSデータセットのシーンごとの結果**:
- ほぼすべてのシーンでAST適用時に優れた指標を達成
- 実世界データセットでのASTの汎用性を実証

**HyperNeRFデータセットの結果**:
- カメラポーズが不正確な場合、定量的指標は他手法に劣ることがある
- しかし、視覚的にはアーティファクトが少なく、より鮮明な結果

**レンダリング効率**:
- 点群数が~250k未満の場合、30 FPS以上を達成可能（NVIDIA RTX 3090）

### C. さらなるアブレーション

**ネットワークアーキテクチャ**:
- 提案パイプライン内の構造が最適
- 特徴ベースの構造は採用せず（動的シーンが低ランク仮定に適合しないため）

**背景色**:
- 一般的に黒背景が良好な結果
- 一部シーン（bouncing、trex）では白背景が優れる

**SE(3)変形フィールド**:
- D-NeRFデータセットでわずかな改善
- NeRF-DSデータセットでは品質低下
- トレーニング時間50%増加、レンダリングFPS 20%低下
- 直接加算を採用

### D. 失敗ケース

**不正確なポーズ**:
- 変形可能GSの収束失敗を引き起こす可能性
- 明示的ポイントベースレンダリングでは特に有害

**少数のトレーニング視点**:
- Few-shotと限定視点の二重の課題
- DeVRFデータセットで顕著なオーバーフィッティング
- トレーニングセットとテストセットを入れ替えると改善

---

## まとめ

本論文は、3D Gaussian Splattingを動的シーンに拡張する先駆的な研究である。変形フィールドとアニーリングスムージングトレーニングを組み合わせることで、高品質かつリアルタイムな動的シーン再構成を実現している。実世界データでの課題（ポーズ推定の不正確さ、視点の制限）に対する解決策も提示しており、AR/VR、3Dコンテンツ制作などの応用分野に大きな可能性を示している。
