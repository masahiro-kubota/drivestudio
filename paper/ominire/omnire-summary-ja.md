# OmniRe - 日本語要約

## 論文情報

**タイトル**: OmniRe: Omni Urban Scene Reconstruction
**著者**: Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li Song, Yue Wang
**所属**: 上海交通大学、Technion、トロント大学、スタンフォード大学、NVIDIA Research、南カリフォルニア大学

---

## Abstract（概要）

本論文は、オンデバイスログから動的な実世界シーンの高忠実度デジタルツインを効率的に作成する包括的なシステム**OmniRe**を導入している。

### 従来手法の課題
- Neural FieldsやGaussian Splattingを使用した既存手法は主に車両に焦点を当てている
- 人間の行動シミュレーションなどの下流アプリケーションに必要な動的要素の包括的フレームワークが欠如

### OmniReの特徴
車両モデリングを超えて、都市シーンにおける多様な動的オブジェクトの正確で完全な再構成を実現：
- 3DGS上にシーングラフを構築
- 正準空間とモデルにおける複数のガウシアン表現を構築
- 車両、歩行者、自転車、その他の動的アクターをモデル化

### 主な成果
- 人間が参加するシナリオを含む高度なシミュレーション（~60 Hz）を可能にする
- 歩行者の行動シミュレーションと人間と車両の相互作用
- Waymoデータセットでの広範な評価で、既存のSOTA手法を量的・質的に大幅に上回る
- 5つの追加ドライビングデータセットで汎用性を実証

---

## 1. Introduction（導入）

### 背景と動機

**デジタルツインの重要性**:
- 高忠実度シミュレーション
- ロバストなアルゴリズムのトレーニングと評価
- 自動運転がエンドツーエンドモデルを採用するにつれて、スケーラブルで高忠実度のシミュレーション環境の需要が増加

**既存技術**:
- NeRF（Neural Radiance Fields）とGaussian Splatting（GS）は、高レベルの視覚的・幾何学的忠実度でシーンを再構成する強力なツール
- しかし、動的都市シーンの正確かつ包括的な再構成は依然として困難
  - 多様な動的アクター
  - 複雑な剛体・非剛体運動

### 従来手法の分類と課題

**カテゴリ1: 動的アクターを無視**:
- 静的部分のみを再構成（Tancik et al., 2022; Martin-Brualla et al., 2021など）

**カテゴリ2: 静的・動的フィールドの組み合わせ**:
- D²NeRF, SUDS, EmerNeRFなど
- すべての動的要素を単一の動的フィールドでモデル化
- 問題: 制御性の欠如、センサーシミュレーターとしての実用性が限定的

**カテゴリ3: シーングラフの構築**:
- NSG（Neural Scene Graphs）、UniSim、MARS、StreetGaussiansなど
- バウンディングボックスで動的アクターと静的背景をノードとして表現
- 問題: 主に剛体オブジェクトに焦点、VRU（脆弱な道路利用者）である歩行者や自転車を無視

### OmniReの貢献

**目標**:
すべての動的アクター（車両、歩行者、自転車など）をインタラクティブシミュレーションを可能にする方法でモデル化

**2つの主要課題**:
1. 多様な非剛体動的アクターをモデル化するための包括的フレームワークの開発
2. 人間への特別な焦点（意思決定に重要、関節レベルの再構成が必要）

**人間モデリングの課題**:
- センサー観測の限界（オクルージョン、雑然とした環境）
- 既存の人間ポーズ予測モデルの制限
- スパースセンサーデータからの高忠実度外観再構成
- 車椅子やベビーカーなどの大型機器との相互作用

### OmniReのソリューション

**アーキテクチャ**:
1. **動的ニューラルシーングラフ**（Ost et al., 2021に基づく）:
   - 3D Gaussian Splatting（Kerbl et al., 2023）をベース
   - 異なる動的アクターフィールド用の専用ガウシアン表現
   - ローカル正準空間での表現

2. **表現戦略**:
   - **背景と車両**: 静的ガウシアン、車両は剛体変換で運動をシミュレート
   - **非剛体アクター（歩行者）**: SMPLモデルを組み込んで関節レベルの制御を実現
   - **テンプレートレス動的アクター**: 共有変形フィールドアプローチ

3. **人間ポーズ推定パイプライン**:
   - マルチカメラセットアップとWildでの厳しいオクルージョンに特化
   - ドライビングログ専用に設計

**主な特徴**:
- すべての非剛体カテゴリの統一表現
- 歩行者の特殊な関節レベル制御
- シーン内の関心オブジェクトの正確な表現と制御可能な再構成
- AVシミュレーションで一般的に使用される行動・アニメーションモデルと直接互換

### 貢献のまとめ

1. **包括的フレームワーク - OmniRe**:
   - 動的カテゴリカバレッジと表現柔軟性の「omni」原則を体現
   - ガウシアン表現ベースの動的ニューラルシーングラフ
   - 静的背景、駆動車両、非剛体動的アクターの統一再構成
   - 歩行者行動と人間・車両相互作用を含む高忠実度シーン再構成（§4）
   - 既存手法では実現できない能力（§5）

2. **Wildでの人間・動的アクターモデリングの課題への対処**:
   - オクルージョン、雑然とした環境、既存の人間ポーズ予測モデルの制限（§4.2）
   - 5つの追加ドライビングデータセットでの実証

3. **広範な実験とアブレーション**:
   - SOTA性能の達成
   - フル画像メトリクス: +1.88 PSNR（再構成）、+2.38 PSNR（NVS）
   - 動的アクター: 車両 +1.18 PSNR、人間 +4.09 PSNR（再構成）、+3.06 PSNR（NVS）

---

## 2. Related Work（関連研究）

### Dynamic Scene Modeling（動的シーンモデリング）

**ニューラル表現の進化**:
- NeRF以降、Novel View Synthesis（NVS）を支配
- 動的シーン再構成への拡張が多様化

**アプローチの分類**:

1. **Deformation-based approaches**（変形ベースアプローチ）:
   - Pumarola et al., DeformableGS, Wu et al.
   - 正準空間を使用、変形ネットワークで時間依存観測をマッピング
   - 問題: 小規模シーンや限定的な動きに制限、都市動的シーンには不向き

2. **Modulation-based techniques**（モジュレーションベース）:
   - 画像タイムスタンプ（または潜在コード）をニューラル表現への追加入力として使用
   - Xian et al., Li et al., Luiten et al.
   - 問題: 制約が不足、追加の監視（オプティカルフローやマルチビュー入力）が必要

3. **Static-Dynamic Decomposition**（静的・動的分解）:
   - D²NeRF: シーンを静的・動的フィールドに分割
   - SUDS, EmerNeRF: 動的自動運転シーンで優れた再構成能力
   - 問題: すべての動的要素を単一動的フィールドでモデル化、制御性の欠如

4. **Scene Graph Approaches**（シーングラフアプローチ）:
   - NSG（Neural Scene Graphs）: シーンを個別のエージェントに明示的に分解
   - バウンディングボックスでシーングラフのノードとして表現
   - UniSim, MARS, NeRF-DS, ML-NSG, StreetGaussians, DrivingGaussians, HUGSなどで広く採用
   - 問題: 剛体オブジェクトのみ対応（時間非依存表現または変形ベース技術の制限）

**最近の研究**:
- Fischer et al. (2024a): 変形フィールドを使用した非剛体モデリングを検討
- ただし、柔軟な制御を可能にする詳細な人間モデルには対応していない

**OmniReの位置づけ**:
剛体・非剛体オブジェクトの両方に様々なガウシアン表現を組み込んだガウシアンシーングラフを提案し、多様なアクターに追加の柔軟性と制御性を提供。

### Human Modeling（人間モデリング）

**課題**:
- 可変外観と複雑な動き
- 専用のモデリング技術が必要

**既存手法**:

1. **SMPLベースの手法**:
   - NeuMan（Jiang et al., 2022）: SMPLボディモデルを使用して正準空間で生ポイントをワープ
   - 非剛体人体の再構成と細かい制御を可能にする
   - GART（Lei et al., 2023）, GauHuman（Hu & Liu, 2023）, HumanGaussians（Kocabas et al., 2023）: ガウシアン表現とSMPLモデルを組み合わせ
   - 問題: Wildでは直接適用できない

2. **ドライビングシーン向け**:
   - Yang et al. (2021): 形状とポーズ再構成、LIDARシミュレーションに焦点
   - Wang et al. (2023; 2024): 部分観測から自然で正確な人間の動きを再現
   - 問題: 形状とポーズ再構成、外観モデリングにのみ焦点

**OmniReの位置づけ**:
- 人間の外観をモデル化するだけでなく、包括的なシーンフレームワーク内でこのモデリングを統合
- 都市シーンの課題（多数の歩行者、スパース観測、厳しいオクルージョン）に対処（§4.2で詳述）

---

## 3. Preliminaries（前提知識）

### 3D Gaussian Splatting（3Dガウシアンスプラッティング）

**概要**:
- Kerbl et al. (2023)で初めて導入
- 色付きブロブのセット $\mathcal{G} = \{g\}$ でシーンを表現
- 強度分布がガウシアン

**各ガウシアン（ブロブ）のパラメータ**:
```
g = (o, μ, q, s, c)
```
- o: 不透明度 ∈ (0, 1)
- μ: 平均位置 ∈ ℝ³
- q: 回転（四元数） ∈ ℝ⁴
- s: 異方性スケーリング因子 ∈ ℝ³₊
- c: 視点依存色（球面調和関数係数） ∈ ℝᶠ

**ピクセル色の計算**:
```
C = Σ Π(1-αⱼ) · cᵢ
```
- カメラ中心からの距離でガウシアンをソート
- αブレンディング

**剛体変換の定義**:
```
T ⊗ G = (o, Rμ + t, Rot(R, q), s, c)
```
- T = (R, t) ∈ SE(3)
- Rot(·): 回転行列で四元数を回転

### Skinned Multi-Person Linear (SMPL) Model

**概要**:
- Loper et al. (2015)で提案
- 三角形メッシュとLBS（Linear Blending Skinning）の利点を組み合わせ
- 体の形状とポーズを操作

**テンプレートメッシュ**:
```
M_h = (V, F)
```
- V ∈ ℝⁿᵛˣ³: nᵥ個の頂点
- 正準レストポーズで定義

**形状とポーズのパラメータ化**:
```
V_S = V + B_S(β) + B_P(θ)
```
- β: 形状パラメータ
- θ: ポーズパラメータ
- B_S, B_P: 個々の頂点へのxyzオフセット

**LBS（Linear Blend Skinning）**:
```
v'ᵢ = (Σ Wₖ,ᵢGₖ) vᵢ
```
- W ∈ ℝⁿᵛˣⁿᵏ: 事前定義のLBS重み
- G: 関節変換
- nₖ: 関節数

**ポーズパラメータ**:
- θ_b ∈ ℝ²³ˣ³ˣ³: ボディポーズ成分
- θ_t ∈ ℝ³ˣ³: グローバルポーズ（移動）

**OmniReでの使用**:
- 各歩行者のすべてのフレームでポーズパラメータθを取得
- 個々のボディ形状パラメータβ⟨ʰ⟩を取得
- ポーズシーケンスで歩行者の非剛体ダイナミクスを初期化（§4.2で詳述）

---

## 4. Method（手法）

### 概要（Figure 2参照）

静的背景と多様な可動エンティティの両方を包括的に再構成する3DGSフレームワークを構築。

**§4.1**: 異なるセマンティッククラスを多様なガウシアン表現で表す体系的アプローチ
- この複雑かつ効率的なシステムレベルフレームワークが主要な貢献の1つ

**§4.2**: 制約のない環境での人間モデリング
- 人間の動きの複雑性
- Wildでの厳しいオクルージョンによる幾何と外観の正確なモデリングの困難さ
- 既存のシーングラフベース手法からフレームワークを大幅に拡張

**§4.3**: 完全なシーンの最適化方法

**トレーニング後**:
- シーン内のすべての可動要素の忠実で制御可能な再構成を取得
- 高度なシミュレーションとインタラクティブシナリオを可能にする

### 4.1. Dynamic Gaussian Scene Graph Modeling（動的ガウシアンシーングラフモデリング）

#### Gaussian Scene Graph（ガウシアンシーングラフ）

**目的**:
再構成品質を犠牲にすることなく、多様な可動オブジェクトの柔軟な制御を実現

**ノードの構成**:

1. **Sky Node**（空ノード）:
   - エゴカーから遠く離れた空を表現
   - 最適化可能な環境テクスチャマップ
   - Chen et al. (2023)と類似
   - 個別にレンダリング、αブレンディングで合成

2. **Background Node**（背景ノード）:
   - 建物、道路、植生などの静的シーン背景
   - 静的ガウシアンのセット $\mathcal{G}^{\text{bg}}$
   - 3DGSポイントと追加ランダムポイントを蓄積して初期化（Chen et al., 2023の戦略）

3. **Rigid Nodes**（剛体ノード）:
   - 車両やトラックなど剛体で移動可能なオブジェクト
   - オブジェクトのローカル空間で $\mathcal{G}_v^{\text{rigid}}$ として定義
   - ローカル空間では時間経過で変化しない
   - ワールド空間では車両のポーズ $\mathbf{T}_v \in \mathbb{SE}(3)$ に応じて変化

   **変換式**:
   ```
   G_v^rigid(t) = T_v(t) ⊗ G_v^rigid
   ```

4. **Non-Rigid Nodes**（非剛体ノード）:
   - 歩行者や自転車など
   - 従来手法では見過ごされてきた（Zhou et al., 2023; Yan et al., 2024など）
   - 人間中心シミュレーションには重要

   **2つのサブカテゴリ**:

   a) **SMPL Nodes**（SMPLノード）:
      - 歩行またはランニング中の歩行者
      - テンプレート付き、関節レベルの制御を可能にする
      - モデルパラメータ (θ(t), β) を使用して3Dガウシアンを駆動
      - θ(t) ∈ ℝ²⁴ˣ³ˣ³: 時間変化する人間の姿勢

      **実装**:
      - SMPLテンプレートメッシュ $\mathcal{M}_h$ をレスティングポーズ（Daポーズ）から3Dガウシアンでテッセレート
      - Lei et al. (2023)と類似の戦略
      - 各ガウシアンを $\mathcal{M}_h$ の対応する頂点にバインド

      **ワールド空間ガウシアンの計算**:
      ```
      G_h^SMPL(t) = T_h(t) ⊗ LBS(θ(t), G_h^SMPL)
      ```
      - T_h(t) ∈ SE(3): 時刻tでのノードのグローバルポーズ
      - LBS(·): 線形ブレンドスキニング操作

      **課題**:
      - スパースマルチパーソンまたは屋内シナリオから $\theta(t)$ を最適化するのは非常に困難
      - 粗い初期化が必要（§4.2で詳述）

   b) **Deformable Nodes**（変形可能ノード）:
      - SMPLモデリングの範囲外の重要な非剛体インスタンス
        - 非常に遠い歩行者（SOTA 3D/4D人間推定器でも正確な推定不可）
        - 分布外、テンプレートレスの非剛体個体

      **アプローチ**:
      - 一般的な変形ネットワーク $\mathcal{F}_\varphi$（パラメータ $\varphi$）を使用
      - ノード内の非剛体運動を学習

      **ワールド空間ガウシアンの定義**:
      ```
      G_h^deform(t) = T_h(t) ⊗ (G_h^deform ⊕ F_φ(G_h^deform, e_h, t))
      ```
      - 変形ネットワークが時刻tから正準空間ガウシアンへのガウシアン属性の変化を生成
      - δμ_h(t): 位置の変化
      - δq_h(t): 回転の変化
      - δs_h(t): スケーリング因子の変化
      - ⊕演算子: 単純な算術加算を内部的に実行

      **従来手法との違い**:
      - Yang et al. (2023c): シーン全体に単一の変形ネットワーク → 多くの動きを含む複雑なシーンで失敗
      - OmniRe: ノードごとの変形フィールド → より多くの表現力

      **効率性の維持**:
      - ネットワーク重みはそのアイデンティティを共有
      - ノードはインスタンス埋め込みパラメータ $\mathbf{e}_h$ で曖昧さ解消
      - §5.2で変形可能ガウシアンが良好な再構成品質に不可欠であることを実証

#### Sky Node（空ノード）

**実装**:
- 視方向から空の色をフィットさせるための個別最適化可能な環境マップ

**最終レンダリング**:
```
C = C_G + (1 - O_G)|C_sky
```
- C_sky: 空画像
- C_G: レンダリングされたガウシアン（$\mathcal{G}^{\text{bg}}, \{\mathcal{G}_v^{\text{rigid}}\}, \{\mathcal{G}_h^{\text{SMPL}}\}, \{\mathcal{G}_h^{\text{deform}}\}$）
- O_G: ガウシアンのレンダリングされた不透明度マスク

### 4.2. Reconstructing In-the-Wild Humans（Wildでの人間再構成）

#### 課題

**Wildポーズ推定器の問題**:
- 通常、単一ビデオ入力用に設計（Goel et al., 2023; Rajasegaran et al., 2022）
- オクルージョンケースで予測を見逃すことが多い

**提案ソリューション**:
頻繁なオクルージョンを伴うマルチビュービデオから正確で時間的に一貫した人間のポーズを予測するパイプライン

#### 問題の形式化

**入力**:
- N人の3Dトラックレット: $\{\mathbf{T}_h, \mathbf{b}_h\}_{h=1}^{N-1}$（データセットから）

**目標**:
- 対応するSMPLポーズセットを取得: $\boldsymbol{\theta} = \{\boldsymbol{\theta}_h\}_{h=1}^{N-1}$

**記法**:
- T_h: h番目の人間のボックスシーケンス
- θ_h: h番目の人間のボディポーズシーケンス
- 簡潔のため ⟨h⟩ を省略

#### アプローチ

**ステップ1: マルチカメラ処理**:
- マルチカメラセットアップで各カメラのビデオに4D-Humans（Goel et al., 2023）を独立に適用
- 個別に処理された人間トラックレットとポーズの結果を生成:
  ```
  T = ⋃ T^c, θ = ⋃ θ^c
  ```
  - T^c, θ^c: カメラcからの予測トラックレットとポーズ
  - D^c: カメラcで検出された人間インデックスのセット

**ステップ2: タスク**:
- $\hat{\boldsymbol{\theta}}$ を使用して $\boldsymbol{\theta}$ を再構成

#### 手法の詳細

**a) Tracklet Matching（トラックレットマッチング）**:

**マッチング関数**:
```
θ̂_h = M(h, θ̂, T, T̂)
```
- 各グラウンドトゥルーストラックレットに対して最も類似した予測トラックレットを見つける
- 2D投影の最大平均IoUを計算

**例（3カメラセットアップ）**（Figure 3(a)参照）:
- h番目のグラウンドトゥルーストラックレットがカメラ0〜2で予測トラックレット j₀, j₁, j₂ とマッチした場合:
  ```
  θ̂_h = {θ^0_{j₀}, θ^1_{j₁}, θ^2_{j₂}}
  ```

**b) Pose Completion（ポーズ補完）**（Figure 3(b)参照）:

**問題**:
4D-Humans（Goel et al., 2023）はドライビングシナリオでオクルージョンされた個体のSMPLポーズ予測に失敗

**ソリューション**:
欠落ポーズを回復するプロセスを設計:
```
θ_h = H(θ̂_h, T, T̂)
```
- 関数H: グラウンドトゥルースと予測トラックレットを比較して欠落検出を識別
- 欠落ポーズを補間して $\theta_h$ を $\hat{\theta}_h$ から完成

### 4.3. Optimization（最適化）

#### 最適化パラメータ

**単一ステージですべてのパラメータを同時最適化**:

1. **ガウシアン属性**（ローカル空間）:
   - 不透明度、平均位置、スケーリング、回転、外観
   - $\mathcal{G}^{\text{bg}}, \{\mathcal{G}_v^{\text{rigid}}\}, \{\mathcal{G}_h^{\text{SMPL}}\}, \{\mathcal{G}_h^{\text{deform}}\}$

2. **ノードのポーズ**（各フレームt）:
   - 剛体・非剛体ノード: $\{\mathbf{T}_v(t)\}, \{\mathbf{T}_h(t)\}$

3. **SMPLノードの人間ポーズ**（各フレームt）:
   - $\{\boldsymbol{\theta}_h(t)\}$ と対応するスキニング重み

4. **変形ネットワークの重み**:
   - $\varphi$ (変形ネットワーク $\mathcal{F}_\varphi$)

5. **空モデルの重み**

#### 目的関数

```
L = (1-λ_s)L_1 + λ_s L_SSIM + λ_depth L_depth + λ_opacity L_opacity + L_reg
```

**各損失項の説明**:

1. **L_1, L_SSIM**: レンダリング画像に対するL1とSSIM損失

2. **L_depth**: レンダリングされたガウシアンの深度とLiDARからのスパース深度信号を比較

3. **L_opacity**: ガウシアンの不透明度を非空マスクと整合させる

4. **L_reg**: 異なるガウシアン表現に適用される様々な正則化項

**詳細な損失項の説明は付録に記載**

---

## 5. Experiments（実験）

### データセットとセットアップ

**メインデータセット**:
- Waymo Open Dataset（Sun et al., 2020）
- 実世界ドライビングログで構成
- 最大32の動的シーンでテスト
- 8つの高度に複雑な動的シーン（一般的な車両に加えて多様な動的クラス）
- 各セグメント約150フレーム
- セグメントID: Tab. 12およびTab. 6に記載

**追加データセット**（汎用性実証のため）:
1. NuScenes (Caesar et al., 2020)
2. Argoverse2 (Wilson et al., 2023)
3. PandaSet (Xiao et al., 2021)
4. KITTI (Geiger et al., 2012)
5. NuPlan (Caesar et al., 2021)

### ベースライン

**Gaussian Splattingアプローチ**:
- 3DGS (Kerbl et al., 2023)
- DeformableGS (Yang et al., 2023c)
- StreetGS (Yan et al., 2024)
- HUGS (Zhou et al., 2024)
- PVG (Chen et al., 2023)

**NeRFベースアプローチ**:
- EmerNeRF (Yang et al., 2023a)

**実装詳細**:
- 3DGSとDeformableGS: LiDAR深度監視を含む実装を使用（公平な比較のため）
- その他: 公式コードを使用
- トレーニングデータ: 3台の前面カメラ、640×960にリサイズ、LiDARデータで監視
- データセット提供のインスタンスバウンディングボックスを使用
- トレーニング中にポーズ最適化で微調整

### 5.1. Main Results（主な結果）

#### Appearance（外観）

**評価設定**:
- シーン再構成とNVS（Novel View Synthesis）タスクで評価
- NVS: 10フレームごとをホールドアウトセットとして使用
- メトリクス: PSNR、SSIM
  - フル画像
  - 人間関連領域
  - 車両関連領域（動的再構成能力を評価）

**定量的結果（Table 1）**:
- **OmniReがすべての手法を上回る**
- **人間関連領域で特に大きなマージン**
  - 動的アクターの包括的モデリングを検証

**StreetGSとの比較**:
- StreetGS（Yan et al., 2024）とOmniReは車両を類似の方法でモデル化
- **OmniReが車両領域でもわずかに優位**
- 理由: StreetGSには人間モデリングがない
  - 人間領域からの監視信号（色、LiDAR深度）が誤って車両モデリングに影響
  - これがシーンを包括的にモデル化する動機の1つ
  - 誤った監視と意図しない勾配伝播を排除

**定性的結果（Figure 4）**:

1. **PVG (Chen et al., 2023)**:
   - シーン再構成タスクでは良好
   - 高度に動的なシーンでのNVSタスクで苦戦
   - 新規ビューで動的オブジェクトがぼやける（Fig. 4-(f)）

2. **HUGS (Zhou et al., 2024)** (Fig. 4-(e)):
   - 非剛体オブジェクトをモデル化できないため、詳細を再構成できない

3. **StreetGS (Yan et al., 2024)** (Fig. 4-(d)):
   - 同様に非剛体オブジェクトをモデル化できない

4. **3DGS (Kerbl et al., 2023)** (Fig. 8-(a)):
   - 同様の問題

5. **DeformableGS (Yang et al., 2023c)** (Fig. 8-(g)):
   - 大きな動きのある屋外シーンで極端なモーションブラー
   - 屋内シーンや小さな動きのケースでは妥当な性能

6. **EmerNeRF (Yang et al., 2023a)** (Fig. 4-(c)):
   - 遠距離の歩行者と車両の粗い構造を再構成
   - 細かい詳細では苦戦

**OmniReの優位性**:
- シーンのあらゆる部分の詳細を忠実に再構成
- オクルージョン、変形、極端な動きに対応
- ビデオ比較はプロジェクトページに掲載

#### Geometry（幾何）

**評価**:
- 都市シーンの詳細な幾何を再構成できるかを調査
- メトリクス:
  - RMSE（Root Mean Squared Error）
  - Two-way Chamfer Distance (CD)
- トレーニングフレームと新規フレーム両方でLiDAR深度再構成を評価

**結果（Table 1）**:
- **OmniReが大きなマージンで他手法を上回る**
- Figure 5: 他アプローチと比較して動的アクターの正確な再構成を示す

### 5.2. Ablation Studies & Applications（アブレーション研究と応用）

#### a) SMPL Modeling（SMPLモデリング）

**重要性**:
人間の局所的で連続的な動きをモデル化するために重要

**実験（Table 2）**:
- SMPLによる人間ポーズ変形を無効化
- 結果: (a) v.s. (b)

**観察（Figure 7-B）**:
- **テンプレートベースモデリングなし**:
  - 再構成された人間の動作がぼやける（特に脚の周り）
  - 人体の動きを正確に再構成できない

- **デフォルト設定（SMPLあり）**:
  - 精密な脚の再構成を観察
  - 関節レベルの制御を提供（Fig. 1-(c,3), (c,4)）

#### b) Human Body Pose Refinement（人体ポーズの改良）

**問題**:
- §4.2で抽出した人体ポーズに予測エラーとスケールの曖昧さ
- 再構成品質を低下させるポーズエラー（Fig. 6 - Noisy）

**ソリューション**:
- 同じ再構成損失を介して人間のポーズとガウシアンを共同最適化

**結果**:
- Fig. 6-(a): 設計選択を示す
- Fig. 6: 改良されたポーズを示す
- 改良戦略の有効性を検証

#### c) Deformable Nodes（変形可能ノード）

**重要性**:
分布外またはテンプレートレスアクターを正確に再構成するために重要

**アプローチ**:
正準空間からシェイプ空間へガウシアンを変換する自己教師あり変形フィールドを学習

**結果（Table 2）**:
- (a) v.s. (d): このコンポーネントの重要性を証明

**観察（Figure 7-A）**:
- **変形可能ノードなし**: 一部の動的アクターが無視されるか、誤って背景にブレンド
- **変形可能ノードあり**: 適切にモデル化

#### d) Boxes Refinement（ボックス改良）

**問題**:
データセット提供のインスタンスバウンディングボックスが不正確

**影響**:
ノイズの多いグラウンドトゥルースボックスがレンダリング品質に有害

**ソリューション**:
トレーニング中にバウンディングボックスパラメータを共同改良

**結果**:
- Table 4: 定量的改善
- Figure 12: 視覚的な実際的利点

#### e) Applications to Simulation（シミュレーションへの応用）

**OmniReの分解の性質**:
各インスタンスが個別にモデル化される

**可能な操作**（共同トレーニング後）:
1. **包括的に再構成されたアセット**:
   - 位置と回転を柔軟に編集可能

2. **シーン内編集**:
   - 単一シーン内での編集に留まらない

3. **シーン間アセット転送**:
   - あるシーンから別のシーンへアセットを転送
   - 再構成環境に多様性と複雑性を追加

**実例（Figure 1-(c)）**:
- **(c,left)**: シーン内の元の黒い車両を別のシーンから再構成した車両に入れ替え
- **(c,right)**: インセット内のあるシーンから歩行者を通りに挿入、移動車に出会わせる

**追加ユースケース**（Figure 11）:
- 詳細な歩行者・車両相互作用を含むシナリオのシミュレーション
- Fig. 9: 横断歩道で停止する移動車両、ゆっくり横断する歩行者を待つ
  - 歩行者は別のシーンから精密に再構成
  - 過去のシミュレーター（Wang et al., 2022; Wei et al., 2024）では実現困難な、再構成されたフォトリアリスティックアセットに対する精密な制御

---

## 6. Conclusion（結論）

### 成果のまとめ

**OmniReの特徴**:
- **Gaussian Scene Graphs**を使用した包括的な都市シーンモデリングに対応
- **高速で高品質**な再構成とレンダリングを達成
- **ドライビングとロボティクスシミュレーション**の可能性を示唆

**複雑環境での人間モデリング**:
- 解決策を提示

### 今後の研究

1. **自己教師あり学習**
2. **改良されたマルチステージトレーニング**
3. **安全性・プライバシーへの配慮**

### 再現性

コードは link で公開（再現性確保のため）

### Broader Impact（より広い影響）

**目的**:
自動運転における重要な問題—シミュレーション—に対処

**潜在的影響**:
- 自律走行車の開発とテストを支援
- より安全で効率的なAVシステムにつながる可能性

**研究課題**:
安全で制御可能なシミュレーションは、依然としてオープンで基本的な研究課題

### Limitations（制限事項）

#### 1. 照明効果

**問題**:
- 照明効果を明示的にモデル化していない
- 異なる照明条件下で再構成された要素を組み合わせる際に視覚的調和の問題が生じる可能性

**影響**:
シミュレーション時の視覚的一貫性の課題

**必要な取り組み**:
- 非自明な課題、現在の研究範囲を超える
- 照明効果のモデリングとシミュレーションリアリズムの向上は今後の重要研究課題

#### 2. カメラポーズの逸脱

**問題**:
- 他のシーンごと最適化手法と同様、カメラがトレーニング軌跡から大幅に逸脱すると、新規ビューの品質が低下

**解決策の方向性**:
- データ駆動事前分布（画像または動画生成モデル）の組み込み
- カメラポーズの共同最適化

---

## 7. Ethics Statement（倫理声明）

### データの取り扱い

**新規データの収集・アノテーションなし**:
- 厳格な倫理ガイドラインに準拠した既存の公開データセットを利用

**データセットの特性**:
- 個人のプライバシー保護のため、識別可能な人間の特徴をぼかすか匿名化

### コミットメント

**責任ある使用**:
- 本手法および将来のアプリケーションが責任を持って倫理的に使用されることを保証
- 安全性の維持とプライバシーの保護

---

## 8. Acknowledgements（謝辞）

**資金提供**:
- Toyota Research Institute
- Dolby
- Google DeepMind

**個人への謝辞**:
- Yue Wang: Powell Faculty Research Awardのサポート
- Jiageng Mao, Junjie Ye, Ziyi Yang, Haozhe Lou, Yifan Lu: プロジェクト中の貴重な議論、問題解決と手法改善への貢献

---

## 補足資料（Supplemental Material）

### A. Implementation Details（実装詳細）

#### 初期化

**背景モデル**:
- PVG (Chen et al., 2023)を参考
- $6 \times 10^5$ LiDARポイント + $4 \times 10^5$ ランダムサンプル
  - $2 \times 10^5$ 近距離サンプル（シーン原点からの距離で均一分布）
  - $2 \times 10^5$ 遠距離サンプル（逆距離で均一分布）
- 動的オブジェクトのLiDARサンプルをフィルタリング

**剛体ノードと非剛体変形可能ノード**:
- 蓄積されたLiDARポイントを利用

**非剛体SMPLノード**:
- 正準空間のテンプレートメッシュ上でガウシアンを初期化
- 画像平面に投影されるガウシアンの初期値を決定（ランダムサンプルはランダムカラーで初期化）
- 初期人体ポーズシーケンス: §4.2で説明したプロセスから取得

#### トレーニング

**イテレーション**:
- 30,000イテレーション
- すべてのシーンノードを共同最適化

**学習率設定**:

1. **ガウシアンプロパティ**:
   - 3DGS (Kerbl et al., 2023)のデフォルト設定に準拠
   - ノードタイプごとにわずかに異なる
   - 回転の学習率:
     - 非剛体SMPLノード: $5 \times 10^{-5}$
     - その他のノード: $1 \times 10^{-5}$

2. **球面調和関数の次数**:
   - 背景、剛体、非剛体変形可能ノード: 3
   - 非剛体SMPLノード: 1

3. **インスタンスボックス**:
   - 回転: $1 \times 10^{-5}$ → $5 \times 10^{-6}$（指数減衰）
   - 移動: $5 \times 10^{-2}$ → $1 \times 10^{-5}$（指数減衰）

4. **人体ポーズ**（非剛体SMPLノード）:
   - $5 \times 10^{-5}$ → $1 \times 10^{-7}$（指数減衰）

**ガウシアン高密度化戦略**:
- Ye et al. (2024)の絶対勾配ガウシアンを利用（メモリ使用量制御のため）
- 位置勾配の高密度化しきい値: $3 \times 10^{-4}$
- 絶対勾配使用による性能への影響は最小（付録D.4参照）
- スケーリングの高密度化しきい値: $3 \times 10^{-3}$

**ハードウェア**:
- NVIDIA RTX 4090 GPU（1台）
- 各シーンのトレーニング時間: 約1時間
- トレーニング設定により変動

#### 最適化

**損失関数**（Eq (7)）:
すべての学習可能パラメータを共同最適化

**1. 画像損失**:
```
L_image = (1-λ_s)L_1 + λ_s L_SSIM
```

**動的部分への対応**:
- 動的部分の時空間観測がスパース → 監視信号が不十分
- 解決策: レンダリングされた動的マスクで識別された動的領域に高い画像損失重みを適用
- 重み: 5

**2. 深度損失**:
```
L_depth = (1/hwc) Σ ||D^s - D̂||₁
```
- D^s: スパース深度マップの逆数
- LiDARポイントを画像平面に投影してスパース LiDARマップを生成
- D̂: 予測深度マップの逆数

**3. マスク損失**:
```
L_opacity = -(1/hw) Σ O_G · log O_G - (1/hw) Σ M_sky · log(1-O_G)
```
- M_sky: 空マスク
- O_G: レンダリングされた不透明度マップ

**4. 正則化項**:
異なるガウシアン表現の品質向上のための様々な正則化項

**重要な正則化項 - L_pose**:
時間経過で滑らかな人体ポーズを確保
```
L_pose = (1/2) ||θ(t-δ) + θ(t+δ) - 2θ(t)||₁
```
- δ: {1, 2, 3, 4, 5}からランダムに選択された整数

**重みの設定**:
- λ_s (SSIM損失): 0.2
- λ_depth (深度損失): 0.1
- λ_opacity (不透明度損失): 0.05
- λ_pose (ポーズ平滑化損失): 0.01

### B. Baselines（ベースライン手法の詳細説明）

詳細な各ベースライン手法の説明が含まれる：
- EmerNeRF
- DeformableGS
- StreetGS
- HUGS
- PVG

### C. Evaluation（評価）

#### Appearance（外観）

**NVSタスク**:
- 元のシーケンスから10フレームごとをテストセットとして選択
- PSNR、SSIMで画質評価

**動的シーン焦点**:
- 車両と人間の領域でPSNR、SSIMを計算
- Segformer (Xie et al., 2021)を使用してセマンティックマスクを取得
- 速度情報を利用した動的マスクで動的オブジェクトを識別
- 動的マスクの例: Fig. 10

#### Geometry（幾何）

**LiDARデータの使用**:
- ガウシアンの初期化
- レンダリングされた深度マップとスパースLiDAR深度マップを比較してシーン深度を監視

**評価方法**（StreetSurf (Guo et al., 2023)に従う）:
- 3Dポイントをレンダリング
- 深度ピクセルを3D LiDAR光線にアンプロジェクト
- Chamfer Distance: 予測深度をLiDAR光線方向と原点を使用して3Dに再投影
- RMSE: LiDAR光線のGT範囲と予測範囲を比較

### D. Additional Results（追加結果）

#### D.1. 定性的比較

プロジェクトページで手法のビデオ比較を推奨

#### D.2. 定量的比較

Waymoデータセットの32動的シーンでStreetGS (Yan et al., 2024)とEmerNeRF (Yang et al., 2023a)との比較（Table 5）

#### D.3. OmniReのチャレンジングシーンでの性能

**a) 超混雑シーン**:
- 極端な動的オクルージョンを持つシーンの再構成に大きな課題
- OmniReは高度にオクルージョンされたシーンでも良好に機能
- 3つの極端に混雑したWaymoシーン（seg112520..., seg104859..., seg152664...）で実験
- Table 7: OmniReがこれらのシーンで強力な性能を示す

**b) 夜間シーン**:
- seg129008..., seg102261..., seg128560... の3つの夜間シーンをテスト
- Table 8: OmniReが比較手法を上回り、低光量条件下で優れた再構成品質を達成

**c) 悪天候条件**:
- 様々な天候タイプの7シーンをテスト:
  - a. rainy (seg113555..., seg109277..., seg141339...)
  - b. foggy (seg161022..., seg172163...)
  - c. cloudy (seg144275..., seg157956...)
- すべての天候条件でOmniReが堅牢に高い再構成忠実度を維持（Table 9）

**d) 高速シーン**:
- 高速道路などの高速シーンで3つの高速道路シーン（seg109239..., seg138396...）をテスト
- Table 10: 高速道路は非剛体オブジェクト（人間など）が通常欠如
- 人間メトリクスは適用不可、非剛体モデリングは不要
- OmniReとStreetGSが同等の性能

#### D.4. アブレーション研究

**a) Absolute Gradient**:
- 3DGS高密度化にAbsGradを適用（StreetGS、DeformableGS、3DGSなど再現手法全体の標準慣行）
- AbsGradの影響を定量化するための比較研究（Table 11）
- AbsGradを無効化すると性能がわずかに低下（約0.1 PSNR）
- DeformableGSはAbsGradなしでメモリ不足の問題
- 結論: AbsGradは3DGS高密度化の良い慣行（+0.1 PSNR）だが、パフォーマンスリードの決定的要因ではない

**b) ネットワークアーキテクチャ**:
- 純粋に暗黙的ネットワークのアーキテクチャに関するアブレーション実験（Table 7）
- パイプライン内の構造が最適であることを示唆
- 特徴ベース構造を採用しない理由: 動的シーンが低ランク仮定に適合しないため

**c) 背景色**:
- NeRFにおいて背景なしシーンレンダリングに黒または白背景を使用するのが一般的
- D-NeRFデータセットの特定シーンで背景色が影響（Table 8）
- 全体的に黒背景がより良い結果
- 一貫性のため、メインテキストの実験ではすべて黒背景を使用
- bouncingとtrexシーンでは白背景がより良い結果

**d) SE(3)変形フィールド**:
- Nerfies (30)に触発され、3Dガウシアン位置の変換に回転を考慮した6-DOF SE(3)フィールドを適用
- 実験結果（Table 5、Table 6）:
  - D-NeRFデータセットでわずかな改善
  - より複雑な実世界NeRF-DSデータセットでは品質低下
  - SE(3)フィールドによる追加計算オーバーヘッド: トレーニング時間約50%増加、レンダリングFPS約20%低下
- 結論: SE(3)制約なしの直接加算を利用

### E. OmniRe In Practice（OmniReの実践）

#### Bounding Boxes（バウンディングボックス）

**他のシーングラフベースアプローチとの類似性**:
- ドライビングシーンモデリングにバウンディングボックスを利用
- Ost et al., 2021; Yang et al., 2023b; Tonderski et al., 2024; Fischer et al., 2024b; Zhou et al., 2023; 2024; Yan et al., 2024と同様

**重要な制御性**:
- 非剛体オブジェクト（車両、人体の動き）の管理に重要
- シーンシミュレーションのようなタスクに不可欠
  - すべての参加エージェントの動きをきめ細かく制御する必要
  - ラストマイルで、バウンディングボックスアノテーションを効率的かつアクセス可能にする

**アノテーション効率**:
- 実世界ドライビングログでは自動ラベリングツールで正確なバウンディングボックスを低コストで生成可能
- 手動作業を最小化し、リソースを効率的かつアクセス可能に

#### 人間のガウシアン表現の決定方法

**2グループへの分類**:

1. **近距離歩行者**:
   - §4.2で紹介した人間ポーズ処理モジュールで検出
   - **変形可能ノード**でモデル化

2. **遠距離歩行者**:
   - 通常、距離のため検出されない
   - **変形可能ノード**でモデル化

**このアプローチの自然な利点**:
- 人間検出能力に基づいて近距離と遠距離の歩行者を自然に区別

**他の個体**:
- 車椅子、スケートボード、自転車などを使用する個体は、一部のケースでデータセット固有のラベルに応じて「サイクリスト」としてラベル付けされることがある
- ただし、これらのラベルはデータセット固有で、他のデータセットに汎化しない可能性

**汎化のための予備実験**:
- GPT-4o (Achiam et al., 2023)を使用して個体（バウンディングボックスでクロップ）を2カテゴリに分類:
  - 個人輸送デバイス（車椅子、自転車、オートバイなど）を使用する歩行者と人間
- 60個体（各カテゴリ30個）でのテスト
- GPT-4o (Achiam et al., 2023)が100%の精度を達成
- 正確なラベルは視覚言語モデルの発展により比較的容易に取得可能

---

## まとめ

本論文は、都市シーンの包括的な動的再構成のための革新的なフレームワーク**OmniRe**を提案している。従来の車両中心のアプローチを超えて、歩行者、自転車などの非剛体アクターを含むすべての動的要素を統一的にモデル化することで、真の意味でのデジタルツインを実現している。

**主な革新点**:
1. Gaussian Scene Graphsによる統一的な表現
2. SMPLモデルによる人間の関節レベル制御
3. マルチカメラ・オクルージョン対応の人間ポーズ推定パイプライン
4. 変形可能ノードによるテンプレートレスアクターのモデル化

**実用的意義**:
- 自動運転シミュレーション
- 人間・車両相互作用の精密な再現
- 安全性テストとアルゴリズム評価

今後の自動運転技術の発展、特に人間中心の安全性評価において、重要な貢献となる研究である。
